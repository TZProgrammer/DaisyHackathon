{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import io\n",
    "from happytransformer import HappyTextClassification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 15 01:08:00 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.77       Driver Version: 512.77       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 18%   40C    P8    17W / 215W |    738MiB /  8192MiB |      6%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1784    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3476    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4036    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12972    C+G   ...\\app-1.0.9008\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     13060    C+G   ....0.15\\OverwolfBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     13604    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13772    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14636    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     17380    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A     19016    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     19760    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     21556    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     21764    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     21928    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     22364    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     23116    C+G   ...Paper_104a\\VideoPaper.exe    N/A      |\n",
      "|    0   N/A  N/A     23736    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     23792    C+G   ...v1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     24092    C+G   ...86)\\Overwolf\\Overwolf.exe    N/A      |\n",
      "|    0   N/A  N/A     25320    C+G   ...ck\\app-4.29.149\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     26176    C+G   ...kzcwy\\mcafee-security.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 629/629 [00:00<00:00, 127kB/s]\n",
      "c:\\Users\\Melina\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Melina\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 268M/268M [00:06<00:00, 44.2MB/s] \n",
      "Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 48.4kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 3.22MB/s]\n",
      "01/15/2023 00:12:38 - INFO - happytransformer.happy_transformer -   Using model: cpu\n",
      "01/15/2023 00:12:38 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"
     ]
    }
   ],
   "source": [
    "happy_tc = HappyTextClassification(\"DISTILBERT\", \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "from happytransformer import TCTrainArgs\n",
    "args = TCTrainArgs(batch_size=8)\n",
    "happy_tc.train(\"train.csv\", args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationResult(label='POSITIVE', score=0.9998737573623657)\n",
      "TextClassificationResult(label='NEGATIVE', score=0.9828665852546692)\n",
      "TextClassificationResult(label='NEGATIVE', score=0.9781290888786316)\n",
      "TextClassificationResult(label='POSITIVE', score=0.9998642206192017)\n",
      "TextClassificationResult(label='NEGATIVE', score=0.9990074038505554)\n"
     ]
    }
   ],
   "source": [
    "print(happy_tc.classify_text(\"I am so happy about this!\"))\n",
    "print(happy_tc.classify_text(\"This is the most dissapointing news I have had in a while.\"))\n",
    "print(happy_tc.classify_text(\"This house was built three years ago.\"))\n",
    "print(happy_tc.classify_text(\"I am amazed by how amazing these nuts are!\"))\n",
    "print(happy_tc.classify_text(\"They have no milk, this sucks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = fasttext.train_unsupervised('data/enwik9')\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        super(TweetRNN, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors) # Replace with embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Look-up the embeddings \n",
    "        x = self.emb(x) \n",
    "        # Set the initial hidden states \n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size) \n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size) \n",
    "        # Forward propagate the RNN \n",
    "        out, __ = self.rnn(x, (h0, c0)) \n",
    "        # Pass the output of the last step to the classifier return \n",
    "        self.fc(out[:,-1,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "965397ff56247331b357ff5da901a682e0db276da67a90def8ca77d3957d3ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
