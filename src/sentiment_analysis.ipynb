{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "#import torchtext\n",
    "import io\n",
    "from happytransformer import HappyTextClassification \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 15 01:34:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.77       Driver Version: 512.77       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 18%   42C    P0    19W / 215W |    720MiB /  8192MiB |     37%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1784    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      3476    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      4036    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12972    C+G   ...\\app-1.0.9008\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     13060    C+G   ....0.15\\OverwolfBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A     13604    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     13772    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14636    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     17380    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A     18568    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     19016    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     19760    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     21556    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     21764    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     21928    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     22504    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     23116    C+G   ...Paper_104a\\VideoPaper.exe    N/A      |\n",
      "|    0   N/A  N/A     23736    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     23792    C+G   ...v1g1gvanyjgm\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     24092    C+G   ...86)\\Overwolf\\Overwolf.exe    N/A      |\n",
      "|    0   N/A  N/A     25320    C+G   ...ck\\app-4.29.149\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     26176    C+G   ...kzcwy\\mcafee-security.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/15/2023 01:35:03 - INFO - happytransformer.happy_transformer -   Using model: cuda\n"
     ]
    }
   ],
   "source": [
    "happy_tc = HappyTextClassification(\"DISTILBERT\", \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "01/15/2023 01:36:04 - INFO - happytransformer.happy_transformer -   Using model: cuda\n",
      "01/15/2023 01:36:04 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n",
      "c:\\Users\\Melina\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 25297\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9489\n",
      "  Number of trainable parameters = 66955779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af5d62263554e9eaa2c135a9ba256c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4645, 'learning_rate': 4.73653704289177e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3689, 'learning_rate': 4.4730740857835393e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3449, 'learning_rate': 4.2096111286753086e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3302, 'learning_rate': 3.946148171567078e-05, 'epoch': 0.63}\n",
      "{'loss': 0.3344, 'learning_rate': 3.682685214458848e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2918, 'learning_rate': 3.419222257350617e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2544, 'learning_rate': 3.155759300242386e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2366, 'learning_rate': 2.8922963431341553e-05, 'epoch': 1.26}\n",
      "{'loss': 0.2415, 'learning_rate': 2.6288333860259252e-05, 'epoch': 1.42}\n",
      "{'loss': 0.232, 'learning_rate': 2.365370428917694e-05, 'epoch': 1.58}\n",
      "{'loss': 0.2398, 'learning_rate': 2.1019074718094636e-05, 'epoch': 1.74}\n",
      "{'loss': 0.2375, 'learning_rate': 1.838444514701233e-05, 'epoch': 1.9}\n",
      "{'loss': 0.207, 'learning_rate': 1.5749815575930024e-05, 'epoch': 2.06}\n",
      "{'loss': 0.1591, 'learning_rate': 1.3115186004847718e-05, 'epoch': 2.21}\n",
      "{'loss': 0.1559, 'learning_rate': 1.0480556433765414e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1613, 'learning_rate': 7.845926862683107e-06, 'epoch': 2.53}\n",
      "{'loss': 0.1359, 'learning_rate': 5.2112972916008005e-06, 'epoch': 2.69}\n",
      "{'loss': 0.1563, 'learning_rate': 2.576667720518495e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2445.3918, 'train_samples_per_second': 31.034, 'train_steps_per_second': 3.88, 'train_loss': 0.2473712116715428, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "happy_tc = HappyTextClassification(\"DISTILBERT\", \"distilbert-base-uncased\",num_labels=3)\n",
    "from happytransformer import TCTrainArgs\n",
    "args = TCTrainArgs(batch_size=8)\n",
    "happy_tc.train(\"train.csv\", args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassificationResult(label='LABEL_2', score=0.9997194409370422)\n",
      "TextClassificationResult(label='LABEL_2', score=0.9994916915893555)\n",
      "TextClassificationResult(label='LABEL_2', score=0.9978398084640503)\n",
      "TextClassificationResult(label='LABEL_2', score=0.9997971653938293)\n",
      "TextClassificationResult(label='LABEL_0', score=0.9892792105674744)\n"
     ]
    }
   ],
   "source": [
    "print(happy_tc.classify_text(\"I am so happy about this!\"))\n",
    "print(happy_tc.classify_text(\"This is the most dissapointing news I have had in a while.\"))\n",
    "print(happy_tc.classify_text(\"This house was built three years ago.\"))\n",
    "print(happy_tc.classify_text(\"I am amazed by how amazing these nuts are!\"))\n",
    "print(happy_tc.classify_text(\"They have no milk, this sucks.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691b339633cf4bf4a4958611d1d70add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bff1b6948046c88cbfe4254d9e438e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcdb0e3b4f54b07909ce712886edd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = fasttext.train_unsupervised('data/enwik9')\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_class):\n",
    "        super(TweetRNN, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors) # Replace with embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Look-up the embeddings \n",
    "        x = self.emb(x) \n",
    "        # Set the initial hidden states \n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size) \n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size) \n",
    "        # Forward propagate the RNN \n",
    "        out, __ = self.rnn(x, (h0, c0)) \n",
    "        # Pass the output of the last step to the classifier return \n",
    "        self.fc(out[:,-1,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "965397ff56247331b357ff5da901a682e0db276da67a90def8ca77d3957d3ab7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
